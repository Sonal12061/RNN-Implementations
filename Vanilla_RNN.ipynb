{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Implementation"
      ],
      "metadata": {
        "id": "S5ZfrLzUsSWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nNIAk6vnN3qw",
        "outputId": "5fd05a71-3720-4a56-9652-acddc396f869"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.0.5\n",
            "    Uninstalling keras-3.0.5:\n",
            "      Successfully uninstalled keras-3.0.5\n",
            "Successfully installed keras-2.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "9127deefd3e8490a86dec1d362a16177"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import reuters"
      ],
      "metadata": {
        "id": "N24-vCRosXdO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset\n",
        "\n",
        "\n",
        "\n",
        "1. Reuters - This dataset is a collection of newswire articles and their corresponding topics.\n",
        "2.  The loading of dataset is done by a utility function provided by the Keras library specifically for loading the Reuters dataset.\n",
        "  *   The function loads the data and returns two tuples.\n",
        "  *   Each element of **X** is a list of word indices representing a document (newswire article), and each element of **y** is an integer representing the category or topic of the corresponding document.\n",
        "  *   The **num_words** parameter in *reuters.load_data(num_words=None, test_split=0.2)* controls the maximum number of words to include in the dataset.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O8naK773taQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = None, test_split = 0.2)"
      ],
      "metadata": {
        "id": "LcfPn3NAsjx5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the dataset"
      ],
      "metadata": {
        "id": "DMBOOIv9vG77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The shape of X_train is {X_train.shape} and the shape of X_test is {X_test.shape}\")\n",
        "print(f\"The shape of X_train is {y_train.shape} and the shape of X_test is {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yir0jF7fvKi7",
        "outputId": "d2abbbf8-2101-4735-d76c-ed3d79dcee3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of X_train is (8982,) and the shape of X_test is (2246,)\n",
            "The shape of X_train is (8982,) and the shape of X_test is (2246,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation of output of X_train[0]\n",
        "\n",
        "The output of *print(X_train[0])* for the Reuters dataset will be the first document of newswire article in the training set represented as a list of word indices. Each index corresponds to a specific word in the vocabulary."
      ],
      "metadata": {
        "id": "kf1mUB_9wSUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4kHeQkbvYuk",
        "outputId": "a208b513-538b-4f6b-d076-3ad60a460f4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking of word indices of the Reuters dataset\n",
        "\n",
        "\n",
        "1.   word_indices is a dict with an example of output as {'mdbl': 10996, 'fawc': 16260, 'degussa': 12089, 'woods': 8803} and so on\n",
        "2.   reverse_word_indices is a dict with an example of output as {10996: 'mdbl', 16260: 'fawc', 12089: 'degussa', 8803: 'woods'} and so on\n",
        "\n"
      ],
      "metadata": {
        "id": "cZBNdMymxBBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_indices = reuters.get_word_index()\n",
        "# word_indices is the dictinory of word - word_index pair.\n",
        "# Example of Words and their respectives indices in the Reuters Dataset\n",
        "for word, index in list(word_indices.items())[:10]:\n",
        "    print(f\"For Word: {word} we have Index: {index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqDfXlTtwEZ2",
        "outputId": "178e7ba2-d3da-4e21-86c5-5281616d95d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Word: mdbl we have Index: 10996\n",
            "For Word: fawc we have Index: 16260\n",
            "For Word: degussa we have Index: 12089\n",
            "For Word: woods we have Index: 8803\n",
            "For Word: hanging we have Index: 13796\n",
            "For Word: localized we have Index: 20672\n",
            "For Word: sation we have Index: 20673\n",
            "For Word: chanthaburi we have Index: 20675\n",
            "For Word: refunding we have Index: 10997\n",
            "For Word: hermann we have Index: 8804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_article_from_indices(word_indices, word_index):\n",
        "    # Create reverse word index dictionary\n",
        "    reverse_word_indices = {value: key for key, value in word_index.items()}\n",
        "    # Decode the article\n",
        "    decoded_article = ' '.join([reverse_word_indices.get(i - 3, '?') for i in word_indices])\n",
        "\n",
        "    return decoded_article"
      ],
      "metadata": {
        "id": "FSHrLSMQ0k65"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_article = decode_article_from_indices(X_train[0], word_indices)\n",
        "print(f\"The Decoded article - {decoded_article}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnMS-ZRn1BTz",
        "outputId": "87114edd-722c-4fb6-ef26-435fd80fdf8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Decoded article - ? mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "ur647LV3-_E1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Data Preprocessing with Tokenization and Binary Vectorization using Keras Tokenizer\n",
        "\n",
        "**Steps:**\n",
        "1.   Importing keras.preprocessing.Tokenizer which is a text preprocessing utility in Keras that is used to tokenize text (split it into words or subwords).\n",
        "2.   **Initialize Tokenizer**: An instance of Tokenizer is created with num_words=100.\n",
        "3. **Convert Sequences to Binary Matrix:**\n",
        "      *   The *sequences_to_matrix method* of the Tokenizer object is used to convert the sequences of word indices *(X_train and X_test)* into binary matrices.\n",
        "      *    In this mode *(mode='binary')*, the matrix representation is such that if a word from the vocabulary exists in a sequence, the corresponding entry in the matrix is set to 1; otherwise, it is set to 0.\n",
        "\n",
        "By performing these steps, the text data is transformed into a format suitable for machine learning models, where each document (news article) is represented as a binary vector indicating the presence or absence of each word from the vocabulary. This preprocessing step is common when working with text data in machine learning tasks. It helps in standardizing the input data and making it compatible with various machine learning algorithms."
      ],
      "metadata": {
        "id": "jFdkFKzL4ig1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)"
      ],
      "metadata": {
        "id": "YFH-UkEa4rz7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have text sequences as your dataset instead of word indices as compared to this dataset you need to tokensize your dataset first and then you need to apply the *sequences_to_matrix* text"
      ],
      "metadata": {
        "id": "TlOTLmyS8RV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = tokenizer.sequences_to_matrix(X_train, mode = 'binary')\n",
        "X_Test = tokenizer.sequences_to_matrix(X_test, mode = 'binary')"
      ],
      "metadata": {
        "id": "kxfNAZZl5GxD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the Tokenized Dataset"
      ],
      "metadata": {
        "id": "YGPm8wly-g3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The shape of the Train tokenized dataset is {X_Train.shape} \\nwhereas the shape of the non-tokenized dataset Train is {X_train.shape}\\n\")\n",
        "print(f\"The shape of the Test tokenized dataset is {X_Test.shape} \\nwhereas the shape of the non-tokenized dataset Test is {X_test.shape}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2THPtFjf8nll",
        "outputId": "b09cef84-2d61-460f-e957-ade379eac912"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the Train tokenized dataset is (8982, 100) \n",
            "whereas the shape of the non-tokenized dataset Train is (8982,)\n",
            "\n",
            "The shape of the Test tokenized dataset is (2246, 100) \n",
            "whereas the shape of the non-tokenized dataset Test is (2246,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The non-tokenized train dataset is {X_train[0]}\\n\")\n",
        "print(f\"The type of the non-tokenized train dataset is {type(X_train[0])}\\n\")\n",
        "print(f\"The shape of the non-tokenized train dataset is {len(X_train[0])}\\n\")\n",
        "print(f\"The tokenized train dataset is {X_Train[0]}\")\n",
        "print(f\"The type of the tokenized train dataset is {type(X_Train[0])}\\n\")\n",
        "print(f\"The shape of the tokenized train dataset is {X_Train[0].shape}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua4yLIez9Iyc",
        "outputId": "022e5430-16ad-40fd-b2fc-29b783f3f32d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The non-tokenized train dataset is [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "\n",
            "The type of the non-tokenized train dataset is <class 'list'>\n",
            "\n",
            "The shape of the non-tokenized train dataset is 87\n",
            "\n",
            "The tokenized train dataset is [0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.\n",
            " 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            " 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "The type of the tokenized train dataset is <class 'numpy.ndarray'>\n",
            "\n",
            "The shape of the tokenized train dataset is (100,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Class Labels to One-Hot Encoded Vectors for Multi-Class Classification using Keras\n",
        "\n",
        "1. **Importing Necessary Function**: The code imports the `to_categorical` function from `keras.utils`. This function is used to convert class labels (integers) into one-hot encoded vectors.\n",
        "\n",
        "2. **Defining Number of Classes**: `num_classes` is set to 46. This typically indicates that there are 46 different classes or categories in the classification problem.\n",
        "\n",
        "3. **Converting Class Labels to One-Hot Encoded Vectors**: The `to_categorical` function is then used to convert the class labels (`y_train` and `y_test`) into one-hot encoded vectors. This means that each class label is converted into a binary vector where the index corresponding to the class is set to 1 and all other indices are set to 0."
      ],
      "metadata": {
        "id": "6BPW-ePt_IpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "num_classes = 46"
      ],
      "metadata": {
        "id": "z_UfCzDl9hVd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_Train = to_categorical(y_train,num_classes)\n",
        "y_Test = to_categorical(y_test,num_classes)"
      ],
      "metadata": {
        "id": "I4KbOqbmAGmI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the categorical dataset"
      ],
      "metadata": {
        "id": "-tHS3BawA1qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The non-categorized train dataset is {y_train[0]}\\n\")\n",
        "print(f\"The type of the non-categorized train dataset is {type(y_train[0])}\\n\")\n",
        "print(f\"The categorized train dataset is {y_Train[0]}\\n\")\n",
        "print(f\"The type of the categorized train dataset is {type(y_Train[0])}\\n\")\n",
        "print(f\"The shape of the total categorized train dataset is {y_Train.shape}\\n\")\n",
        "print(f\"The shape of the categorized train dataset is {y_Train[0].shape}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3EaVxl3AHL_",
        "outputId": "af8e9b3a-f74e-418b-ac5b-a97136e9e877"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The non-categorized train dataset is 3\n",
            "\n",
            "The type of the non-categorized train dataset is <class 'numpy.int64'>\n",
            "\n",
            "The categorized train dataset is [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "The type of the categorized train dataset is <class 'numpy.ndarray'>\n",
            "\n",
            "The shape of the total categorized train dataset is (8982, 46)\n",
            "\n",
            "The shape of the categorized train dataset is (46,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Proper"
      ],
      "metadata": {
        "id": "PcLI4nR-BQaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Necessary Module\n",
        "   - `Sequential`: This is a linear stack of layers in the neural network model.\n",
        "   - `Dense`: This is a fully connected layer.\n",
        "   - `Dropout`: This is a regularization technique that randomly drops a fraction of input units during training to prevent overfitting.\n",
        "   - `Activation`: This specifies the activation function to be applied to the output of the layers.\n",
        "   - `optimizers`: This provides optimization algorithms to train the neural network.\n",
        "   - `pad_sequences`: This is used to pad sequences to the same length."
      ],
      "metadata": {
        "id": "CGbWid3yEBb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, SimpleRNN\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "kMHIKIm2EA6c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Padding Sequences\n",
        "   - `pad_sequences`: This function pads sequences to ensure that they all have the same length. In this case, padding is added at the end of each sequence (`padding='post'`)."
      ],
      "metadata": {
        "id": "7Kw450uUEdVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = pad_sequences(X_Train, padding='post')\n",
        "X_Test = pad_sequences(X_Test, padding='post')"
      ],
      "metadata": {
        "id": "YhX6Q-Y2Etuf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reshaping the data\n",
        "   The input data is reshaped to have a third dimension of 1, which is usually required when using RNN layers in Keras.\n",
        "\n"
      ],
      "metadata": {
        "id": "rmIdYhmDEuZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train = np.array(X_Train).reshape(X_Train.shape[0], X_Train.shape[1], 1)\n",
        "X_Test = np.array(X_Test).reshape(X_Test.shape[0], X_Test.shape[1], 1)"
      ],
      "metadata": {
        "id": "q54qioiuE6QY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining the RNN Model\n",
        "   - This function defines a vanilla RNN model.\n",
        "   - `Sequential()` initializes the model.\n",
        "   - `model.add(SimpleRNN(50, input_shape=(max_words,), return_sequences=False))`: Adds a SimpleRNN layer with 50 units and specifies the input shape. `return_sequences=False` indicates that only the final output of the RNN sequence will be returned.\n",
        "   - `model.add(Dense(num_classes))`: Adds a fully connected layer with `num_classes` units.\n",
        "   - `model.add(Activation['Softmax'])`: Applies the softmax activation function to the output layer.\n",
        "   - `adam = optimizers.Adam(lr=0.001)`: Initializes the Adam optimizer with a learning rate of 0.001.\n",
        "   - `model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])`: Compiles the model with categorical cross-entropy loss function and the Adam optimizer. Accuracy is also specified as a metric.\n"
      ],
      "metadata": {
        "id": "lAU3JmE0E6mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_rnn():\n",
        "  model = Sequential()\n",
        "  model.add(SimpleRNN(50, input_shape=(X_Train.shape[1], X_Train.shape[2]), return_sequences=False))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  adam = optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "bIZecbv7FDHD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Keras Models with scikit-learn Integration\n",
        "\n",
        "1. **Importing `KerasClassifier`**: This code imports the `KerasClassifier` class from `keras.wrappers.scikit_learn` module. `KerasClassifier` allows using Keras models as scikit-learn estimators, enabling seamless integration of Keras models with scikit-learn functionality such as grid search and cross-validation.\n",
        "\n",
        "2. **Creating a KerasClassifier Instance**:\n",
        "   - `model = KerasClassifier(build_fn=basic_rnn, epochs=5, batch_size=50)`: This line creates a `KerasClassifier` instance. It takes several arguments:\n",
        "     - `build_fn`: A function that constructs and returns a Keras model. In this case, `basic_rnn` is assumed to be a function that returns a Keras model.\n",
        "     - `epochs`: The number of epochs (iterations over the entire training dataset) to train the model.\n",
        "     - `batch_size`: The number of samples to use in each training batch.\n",
        "\n",
        "3. **Training the Model**:\n",
        "   - `model.fit(X_Train, y_Train)`: This line trains the model on the provided training data `X_Train` and corresponding labels `y_Train`. The `fit` method adjusts the parameters of the model to minimize the specified loss function, typically using gradient descent or its variants."
      ],
      "metadata": {
        "id": "6NHrG2FYHFXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create a KerasClassifier instance\n",
        "model = KerasClassifier(build_fn=basic_rnn, epochs=50, batch_size=50)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_Train, y_Train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TwzCY-5NByjf",
        "outputId": "4dba8ee2-f890-4002-c6cc-384b1fce81dc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144/144 [==============================] - 5s 27ms/step - loss: 2.5329 - accuracy: 0.4038 - val_loss: 2.1432 - val_accuracy: 0.4513\n",
            "Epoch 2/50\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 2.0915 - accuracy: 0.4607 - val_loss: 2.0372 - val_accuracy: 0.4864\n",
            "Epoch 3/50\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 2.0342 - accuracy: 0.4760 - val_loss: 2.0378 - val_accuracy: 0.4669\n",
            "Epoch 4/50\n",
            "144/144 [==============================] - 4s 30ms/step - loss: 1.9975 - accuracy: 0.4779 - val_loss: 2.0119 - val_accuracy: 0.4919\n",
            "Epoch 5/50\n",
            "144/144 [==============================] - 4s 31ms/step - loss: 1.9744 - accuracy: 0.4825 - val_loss: 2.0328 - val_accuracy: 0.4964\n",
            "Epoch 6/50\n",
            "144/144 [==============================] - 4s 24ms/step - loss: 1.9643 - accuracy: 0.4877 - val_loss: 1.9539 - val_accuracy: 0.4886\n",
            "Epoch 7/50\n",
            "144/144 [==============================] - 5s 33ms/step - loss: 1.9302 - accuracy: 0.4905 - val_loss: 1.9314 - val_accuracy: 0.4953\n",
            "Epoch 8/50\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 1.9283 - accuracy: 0.4928 - val_loss: 1.9329 - val_accuracy: 0.4947\n",
            "Epoch 9/50\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 1.8948 - accuracy: 0.5005 - val_loss: 1.9771 - val_accuracy: 0.4869\n",
            "Epoch 10/50\n",
            "144/144 [==============================] - 6s 41ms/step - loss: 1.8887 - accuracy: 0.5035 - val_loss: 1.8887 - val_accuracy: 0.5064\n",
            "Epoch 11/50\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 1.8744 - accuracy: 0.5080 - val_loss: 1.8932 - val_accuracy: 0.4936\n",
            "Epoch 12/50\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 1.8561 - accuracy: 0.5123 - val_loss: 1.8618 - val_accuracy: 0.5142\n",
            "Epoch 13/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.8474 - accuracy: 0.5159 - val_loss: 1.9132 - val_accuracy: 0.5081\n",
            "Epoch 14/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.8531 - accuracy: 0.5150 - val_loss: 1.8816 - val_accuracy: 0.5064\n",
            "Epoch 15/50\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 1.8269 - accuracy: 0.5254 - val_loss: 1.8392 - val_accuracy: 0.5298\n",
            "Epoch 16/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.8112 - accuracy: 0.5340 - val_loss: 1.8385 - val_accuracy: 0.5248\n",
            "Epoch 17/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.8161 - accuracy: 0.5318 - val_loss: 1.8310 - val_accuracy: 0.5370\n",
            "Epoch 18/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.7776 - accuracy: 0.5388 - val_loss: 1.8218 - val_accuracy: 0.5370\n",
            "Epoch 19/50\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 1.7704 - accuracy: 0.5389 - val_loss: 1.7861 - val_accuracy: 0.5387\n",
            "Epoch 20/50\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 1.7678 - accuracy: 0.5428 - val_loss: 1.8193 - val_accuracy: 0.5320\n",
            "Epoch 21/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.7347 - accuracy: 0.5484 - val_loss: 1.8025 - val_accuracy: 0.5364\n",
            "Epoch 22/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.7279 - accuracy: 0.5521 - val_loss: 1.8310 - val_accuracy: 0.5281\n",
            "Epoch 23/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.7499 - accuracy: 0.5439 - val_loss: 1.8435 - val_accuracy: 0.5225\n",
            "Epoch 24/50\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 1.7315 - accuracy: 0.5499 - val_loss: 1.8170 - val_accuracy: 0.5387\n",
            "Epoch 25/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6915 - accuracy: 0.5633 - val_loss: 1.7464 - val_accuracy: 0.5576\n",
            "Epoch 26/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6886 - accuracy: 0.5623 - val_loss: 1.7633 - val_accuracy: 0.5509\n",
            "Epoch 27/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6833 - accuracy: 0.5652 - val_loss: 1.7616 - val_accuracy: 0.5487\n",
            "Epoch 28/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6750 - accuracy: 0.5680 - val_loss: 1.7423 - val_accuracy: 0.5565\n",
            "Epoch 29/50\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 1.6362 - accuracy: 0.5747 - val_loss: 1.7495 - val_accuracy: 0.5604\n",
            "Epoch 30/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6653 - accuracy: 0.5656 - val_loss: 1.7068 - val_accuracy: 0.5676\n",
            "Epoch 31/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6508 - accuracy: 0.5722 - val_loss: 1.7121 - val_accuracy: 0.5721\n",
            "Epoch 32/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6421 - accuracy: 0.5731 - val_loss: 1.7488 - val_accuracy: 0.5648\n",
            "Epoch 33/50\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 1.6449 - accuracy: 0.5695 - val_loss: 1.7194 - val_accuracy: 0.5604\n",
            "Epoch 34/50\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 1.6241 - accuracy: 0.5761 - val_loss: 1.7113 - val_accuracy: 0.5671\n",
            "Epoch 35/50\n",
            "144/144 [==============================] - 3s 17ms/step - loss: 1.6048 - accuracy: 0.5758 - val_loss: 1.6952 - val_accuracy: 0.5743\n",
            "Epoch 36/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6848 - accuracy: 0.5628 - val_loss: 1.7238 - val_accuracy: 0.5721\n",
            "Epoch 37/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6299 - accuracy: 0.5763 - val_loss: 1.7178 - val_accuracy: 0.5665\n",
            "Epoch 38/50\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 1.5986 - accuracy: 0.5825 - val_loss: 1.6869 - val_accuracy: 0.5821\n",
            "Epoch 39/50\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 1.5932 - accuracy: 0.5827 - val_loss: 1.6821 - val_accuracy: 0.5693\n",
            "Epoch 40/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.5857 - accuracy: 0.5802 - val_loss: 1.7225 - val_accuracy: 0.5632\n",
            "Epoch 41/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6120 - accuracy: 0.5809 - val_loss: 1.6751 - val_accuracy: 0.5748\n",
            "Epoch 42/50\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 1.5976 - accuracy: 0.5815 - val_loss: 1.6830 - val_accuracy: 0.5787\n",
            "Epoch 43/50\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 1.5783 - accuracy: 0.5832 - val_loss: 1.6525 - val_accuracy: 0.5799\n",
            "Epoch 44/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.5687 - accuracy: 0.5898 - val_loss: 1.6583 - val_accuracy: 0.5776\n",
            "Epoch 45/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.5619 - accuracy: 0.5848 - val_loss: 1.7106 - val_accuracy: 0.5687\n",
            "Epoch 46/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.6123 - accuracy: 0.5784 - val_loss: 1.6950 - val_accuracy: 0.5687\n",
            "Epoch 47/50\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 2.1921 - accuracy: 0.4434 - val_loss: 2.1044 - val_accuracy: 0.4446\n",
            "Epoch 48/50\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 1.9675 - accuracy: 0.4917 - val_loss: 1.9103 - val_accuracy: 0.5147\n",
            "Epoch 49/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.8454 - accuracy: 0.5239 - val_loss: 1.9785 - val_accuracy: 0.5053\n",
            "Epoch 50/50\n",
            "144/144 [==============================] - 2s 17ms/step - loss: 1.7953 - accuracy: 0.5338 - val_loss: 1.8358 - val_accuracy: 0.5337\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=<function basic_rnn at 0x781eba430820>\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=50\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=1\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=50\n",
              "\tclass_weight=None\n",
              ")"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=&lt;function basic_rnn at 0x781eba430820&gt;\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=50\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=1\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=50\n",
              "\tclass_weight=None\n",
              ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=&lt;function basic_rnn at 0x781eba430820&gt;\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=50\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=1\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=50\n",
              "\tclass_weight=None\n",
              ")</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Calcualtion of the model"
      ],
      "metadata": {
        "id": "oeTxt4upYaB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model.predict(X_Test)\n",
        "y_test_ = np.argmax(y_Test,axis=1)\n",
        "\n",
        "print(accuracy_score(y_pred,y_Test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SDt02TpHlvl",
        "outputId": "b9f13459-fb54-40cf-f93a-6dfae4b7196b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/45 [==============================] - 0s 6ms/step\n",
            "0.5440783615316117\n"
          ]
        }
      ]
    }
  ]
}
